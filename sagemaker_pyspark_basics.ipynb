{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71b6e1cc",
   "metadata": {},
   "source": [
    "### Goal:\n",
    "- get the basic understandign of how sagemaker_pyspark works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e420184b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: sagemaker_pyspark in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (1.4.5)\n",
      "Requirement already satisfied: pyspark==3.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker_pyspark) (3.3.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker_pyspark) (1.20.3)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pyspark==3.3.0->sagemaker_pyspark) (0.10.9.5)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sagemaker_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0041a1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (22.0.4)\n",
      "Collecting pip\n",
      "  Downloading pip-22.3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.0.4\n",
      "    Uninstalling pip-22.0.4:\n",
      "      Successfully uninstalled pip-22.0.4\n",
      "Successfully installed pip-22.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d36cf5f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install -qU sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1076fe4a",
   "metadata": {},
   "source": [
    "### Load SageMaker Jars programatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c6d6943",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/02 07:21:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "import sagemaker_pyspark\n",
    "\n",
    "conf = (SparkConf()\n",
    "        .set(\"spark.driver.extraClassPath\", \":\".join(sagemaker_pyspark.classpath_jars())))\n",
    "sc=SparkContext(conf=conf)\n",
    "spark=SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "328c109c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::432547830124:role/service-role/AmazonSageMaker-ExecutionRole-20221230T201554\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d1edd5",
   "metadata": {},
   "source": [
    "### Creating Spark Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd14d9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/02 07:29:20 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "23/01/02 07:29:21 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "23/01/02 07:29:21 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-------------------+---------------+\n",
      "|label|            features|distance_to_cluster|closest_cluster|\n",
      "+-----+--------------------+-------------------+---------------+\n",
      "|  5.0|(784,[152,153,154...| 1663.5850830078125|            2.0|\n",
      "|  0.0|(784,[127,128,129...|  1216.953369140625|            5.0|\n",
      "|  4.0|(784,[160,161,162...| 1845.0809326171875|            9.0|\n",
      "|  1.0|(784,[158,159,160...|     1401.056640625|            3.0|\n",
      "|  9.0|(784,[208,209,210...| 1467.3529052734375|            9.0|\n",
      "|  2.0|(784,[155,156,157...| 1716.1195068359375|            6.0|\n",
      "|  1.0|(784,[124,125,126...|     1403.333984375|            3.0|\n",
      "|  3.0|(784,[151,152,153...|  1632.590576171875|            2.0|\n",
      "|  1.0|(784,[152,153,154...| 1153.1788330078125|            3.0|\n",
      "|  4.0|(784,[134,135,161...|      1453.21484375|            0.0|\n",
      "|  3.0|(784,[123,124,125...|  1507.247314453125|            2.0|\n",
      "|  5.0|(784,[216,217,218...|   1611.57763671875|            3.0|\n",
      "|  3.0|(784,[143,144,145...|  1534.952392578125|            1.0|\n",
      "|  6.0|(784,[72,73,74,99...| 1438.5662841796875|            4.0|\n",
      "|  1.0|(784,[151,152,153...|  1126.568603515625|            3.0|\n",
      "|  7.0|(784,[211,212,213...| 1334.6309814453125|            0.0|\n",
      "|  2.0|(784,[151,152,153...| 1644.7357177734375|            6.0|\n",
      "|  8.0|(784,[159,160,161...|  1502.740966796875|            0.0|\n",
      "|  6.0|(784,[100,101,102...| 1535.6138916015625|            4.0|\n",
      "|  9.0|(784,[209,210,211...|   1225.36083984375|            0.0|\n",
      "+-----+--------------------+-------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from sagemaker_pyspark import IAMRole\n",
    "from sagemaker_pyspark.algorithms import KMeansSageMakerEstimator\n",
    "\n",
    "iam_role = \"arn:aws:iam::432547830124:role/service-role/AmazonSageMaker-ExecutionRole-20221230T201554\"\n",
    "\n",
    "region = \"us-east-1\"\n",
    "training_data = spark.read.format(\"libsvm\").option(\"numFeatures\", \"784\")\\\n",
    "  .load(\"s3a://sagemaker-sample-data-{}/spark/mnist/train/\".format(region))\n",
    "\n",
    "test_data = spark.read.format(\"libsvm\").option(\"numFeatures\", \"784\")\\\n",
    "  .load(\"s3a://sagemaker-sample-data-{}/spark/mnist/train/\".format(region))\n",
    "\n",
    "kmeans_estimator = KMeansSageMakerEstimator(\n",
    "    trainingInstanceType=\"ml.m4.xlarge\",\n",
    "    trainingInstanceCount=1,\n",
    "    endpointInstanceType=\"ml.m4.xlarge\",\n",
    "    endpointInitialInstanceCount=1,\n",
    "    sagemakerRole=IAMRole(iam_role))\n",
    "\n",
    "kmeans_estimator.setK(10)\n",
    "kmeans_estimator.setFeatureDim(784)\n",
    "\n",
    "kmeans_model = kmeans_estimator.fit(training_data)\n",
    "\n",
    "transformed_data = kmeans_model.transform(test_data)\n",
    "transformed_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c717fa9",
   "metadata": {},
   "source": [
    "#### References  \n",
    "https://sagemaker-pyspark.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bacdc1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
