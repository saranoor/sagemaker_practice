{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6e8b444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (22.0.4)\n",
      "Collecting pip\n",
      "  Downloading pip-22.3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.0.4\n",
      "    Uninstalling pip-22.0.4:\n",
      "      Successfully uninstalled pip-22.0.4\n",
      "Successfully installed pip-22.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb23877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qU sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ef252e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: sagemaker>2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (2.127.0)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker>2.0) (0.2.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker>2.0) (21.3)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker>2.0) (1.0.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker>2.0) (1.20.3)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker>2.0) (0.2.0)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker>2.0) (0.7.5)\n",
      "Requirement already satisfied: attrs<23,>=20.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker>2.0) (21.2.0)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker>2.0) (4.8.2)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker>2.0) (1.3.4)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker>2.0) (3.19.1)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker>2.0) (0.1.5)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.28 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sagemaker>2.0) (1.26.35)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker>2.0) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.35 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker>2.0) (1.29.42)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker>2.0) (0.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker>2.0) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from packaging>=20.0->sagemaker>2.0) (3.0.6)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker>2.0) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandas->sagemaker>2.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandas->sagemaker>2.0) (2021.3)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pathos->sagemaker>2.0) (1.6.6.4)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pathos->sagemaker>2.0) (0.70.12.2)\n",
      "Requirement already satisfied: pox>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pathos->sagemaker>2.0) (0.3.0)\n",
      "Requirement already satisfied: dill>=0.3.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pathos->sagemaker>2.0) (0.3.4)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from schema->sagemaker>2.0) (21.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from botocore<1.30.0,>=1.29.35->boto3<2.0,>=1.26.28->sagemaker>2.0) (1.26.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U \"sagemaker>2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d23c3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created S3 bucket: sagemaker-us-east-1-432547830124\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sagemaker\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sagemaker_logger = logging.getLogger(\"sagemaker\")\n",
    "sagemaker_logger.setLevel(logging.INFO)\n",
    "sagemaker_logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a2d920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = sagemaker.Session().default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f73e887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-us-east-1-432547830124'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da397259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the dataset from the SageMaker bucket\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3.download_file(\n",
    "    \"sagemaker-sample-files\", \"datasets/tabular/uci_abalone/abalone.csv\", \"abalone.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9f8506d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./preprocess.py\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    OneHotEncoder,\n",
    "    StringIndexer,\n",
    "    VectorAssembler,\n",
    "    VectorIndexer,\n",
    ")\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import (\n",
    "    DoubleType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "\n",
    "\n",
    "def csv_line(data):\n",
    "    r = \",\".join(str(d) for d in data[1])\n",
    "    return str(data[0]) + \",\" + r\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"app inputs and outputs\")\n",
    "    parser.add_argument(\"--s3_input_bucket\", type=str, help=\"s3 input bucket\")\n",
    "    parser.add_argument(\"--s3_input_key_prefix\", type=str, help=\"s3 input key prefix\")\n",
    "    parser.add_argument(\"--s3_output_bucket\", type=str, help=\"s3 output bucket\")\n",
    "    parser.add_argument(\"--s3_output_key_prefix\", type=str, help=\"s3 output key prefix\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"PySparkApp\").getOrCreate()\n",
    "\n",
    "    # This is needed to save RDDs which is the only way to write nested Dataframes into CSV format\n",
    "    spark.sparkContext._jsc.hadoopConfiguration().set(\n",
    "        \"mapred.output.committer.class\", \"org.apache.hadoop.mapred.FileOutputCommitter\"\n",
    "    )\n",
    "\n",
    "    # Defining the schema corresponding to the input data. The input data does not contain the headers\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField(\"sex\", StringType(), True),\n",
    "            StructField(\"length\", DoubleType(), True),\n",
    "            StructField(\"diameter\", DoubleType(), True),\n",
    "            StructField(\"height\", DoubleType(), True),\n",
    "            StructField(\"whole_weight\", DoubleType(), True),\n",
    "            StructField(\"shucked_weight\", DoubleType(), True),\n",
    "            StructField(\"viscera_weight\", DoubleType(), True),\n",
    "            StructField(\"shell_weight\", DoubleType(), True),\n",
    "            StructField(\"rings\", DoubleType(), True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Downloading the data from S3 into a Dataframe\n",
    "    total_df = spark.read.csv(\n",
    "        (\"s3://\" + os.path.join(args.s3_input_bucket, args.s3_input_key_prefix, \"abalone.csv\")),\n",
    "        header=False,\n",
    "        schema=schema,\n",
    "    )\n",
    "\n",
    "    # StringIndexer on the sex column which has categorical value\n",
    "    sex_indexer = StringIndexer(inputCol=\"sex\", outputCol=\"indexed_sex\")\n",
    "\n",
    "    # one-hot-encoding is being performed on the string-indexed sex column (indexed_sex)\n",
    "    sex_encoder = OneHotEncoder(inputCol=\"indexed_sex\", outputCol=\"sex_vec\")\n",
    "\n",
    "    # vector-assembler will bring all the features to a 1D vector for us to save easily into CSV format\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[\n",
    "            \"sex_vec\",\n",
    "            \"length\",\n",
    "            \"diameter\",\n",
    "            \"height\",\n",
    "            \"whole_weight\",\n",
    "            \"shucked_weight\",\n",
    "            \"viscera_weight\",\n",
    "            \"shell_weight\",\n",
    "        ],\n",
    "        outputCol=\"features\",\n",
    "    )\n",
    "\n",
    "    # The pipeline is comprised of the steps added above\n",
    "    pipeline = Pipeline(stages=[sex_indexer, sex_encoder, assembler])\n",
    "\n",
    "    # This step trains the feature transformers\n",
    "    model = pipeline.fit(total_df)\n",
    "\n",
    "    # This step transforms the dataset with information obtained from the previous fit\n",
    "    transformed_total_df = model.transform(total_df)\n",
    "\n",
    "    # Split the overall dataset into 80-20 training and validation\n",
    "    (train_df, validation_df) = transformed_total_df.randomSplit([0.8, 0.2])\n",
    "\n",
    "    # Convert the train dataframe to RDD to save in CSV format and upload to S3\n",
    "    train_rdd = train_df.rdd.map(lambda x: (x.rings, x.features))\n",
    "    train_lines = train_rdd.map(csv_line)\n",
    "    train_lines.saveAsTextFile(\n",
    "        \"s3://\" + os.path.join(args.s3_output_bucket, args.s3_output_key_prefix, \"train\")\n",
    "    )\n",
    "\n",
    "    # Convert the validation dataframe to RDD to save in CSV format and upload to S3\n",
    "    validation_rdd = validation_df.rdd.map(lambda x: (x.rings, x.features))\n",
    "    validation_lines = validation_rdd.map(csv_line)\n",
    "    validation_lines.saveAsTextFile(\n",
    "        \"s3://\" + os.path.join(args.s3_output_bucket, args.s3_output_key_prefix, \"validation\")\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0eb49ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-us-east-1-432547830124'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2540d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "# Upload the raw input dataset to a unique S3 location\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "prefix = \"sagemaker/spark-preprocess-demo/{}\".format(timestamp_prefix)\n",
    "input_prefix_abalone = \"{}/input/raw/abalone\".format(prefix)\n",
    "input_preprocessed_prefix_abalone = \"{}/input/preprocessed/abalone\".format(prefix)\n",
    "\n",
    "sagemaker_session.upload_data(\n",
    "    path='abalone.csv', bucket=sagemaker_session.default_bucket(), key_prefix=input_prefix_abalone\n",
    ")\n",
    "\n",
    "# Run the processing job\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark\",\n",
    "    framework_version=\"3.1\",\n",
    "    role=role,\n",
    "    instance_count=2,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=1200,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2379765",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62691e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating processing-job with name sm-spark-2023-01-04-11-00-58-459\n",
      "INFO:sagemaker:Creating processing-job with name sm-spark-2023-01-04-11-00-58-459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sm-spark-2023-01-04-11-00-58-459\n",
      "Inputs:  [{'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-432547830124/sm-spark-2023-01-04-11-00-58-459/input/code/preprocess.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'output-1', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-432547830124/sagemaker/spark-preprocess-demo/2023-01-04-11-00-52/spark_event_logs', 'LocalPath': '/opt/ml/processing/spark-events/', 'S3UploadMode': 'Continuous'}}]\n",
      "............................................................................!"
     ]
    }
   ],
   "source": [
    "spark_processor.run(\n",
    "    submit_app=\"./preprocess.py\",\n",
    "    arguments=[\n",
    "        \"--s3_input_bucket\",\n",
    "        bucket,\n",
    "        \"--s3_input_key_prefix\",\n",
    "        input_prefix_abalone,\n",
    "        \"--s3_output_bucket\",\n",
    "        bucket,\n",
    "        \"--s3_output_key_prefix\",\n",
    "        input_preprocessed_prefix_abalone,\n",
    "    ],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, prefix),\n",
    "    logs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8465848c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 rows from s3://sagemaker-us-east-1-432547830124/sagemaker/spark-preprocess-demo/2023-01-04-11-00-52/input/preprocessed/abalone/train/\n",
      "5.0,0.0,0.0,0.275,0.195,0.07,0.08,0.031,0.0215,0.025\n",
      "5.0,0.0,0.0,0.29,0.225,0.075,0.14,0.0515,0.0235,0.04\n",
      "7.0,0.0,0.0,0.305,0.225,0.07,0.1485,0.0585,0.0335,0.045\n",
      "7.0,0.0,0.0,0.305,0.23,0.08,0.156,0.0675,0.0345,0.048\n",
      "9.0,0.0,0.0,0.33,0.26,0.08,0.2,0.0625,0.05,0.07\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 5 rows from s3://{}/{}/train/\".format(bucket, input_preprocessed_prefix_abalone))\n",
    "!aws s3 cp --quiet s3://$bucket/$input_preprocessed_prefix_abalone/train/part-00000 - | head -n5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dc11350",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Pulling spark history server image...\n",
      "INFO:sagemaker.spark.processing:Pulling spark history server image...\n",
      "docker command: docker pull 173754725891.dkr.ecr.us-east-1.amazonaws.com/sagemaker-spark-processing:3.1-cpu\n",
      "INFO:sagemaker.local.image:docker command: docker pull 173754725891.dkr.ecr.us-east-1.amazonaws.com/sagemaker-spark-processing:3.1-cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image pulled: 173754725891.dkr.ecr.us-east-1.amazonaws.com/sagemaker-spark-processing:3.1-cpu\n",
      "INFO:sagemaker.local.image:image pulled: 173754725891.dkr.ecr.us-east-1.amazonaws.com/sagemaker-spark-processing:3.1-cpu\n",
      "Error response from daemon: No such container: history_server\n",
      "Error: No such container: history_server\n",
      "History server terminated\n",
      "INFO:sagemaker.spark.processing:History server terminated\n",
      "Starting history server...\n",
      "INFO:sagemaker.spark.processing:Starting history server...\n",
      "History server is up on https://sagemakerpyspark.notebook.us-east-1.sagemaker.aws/proxy/15050\n",
      "INFO:sagemaker.spark.processing:History server is up on https://sagemakerpyspark.notebook.us-east-1.sagemaker.aws/proxy/15050\n"
     ]
    }
   ],
   "source": [
    "spark_processor.start_history_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7be3fdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Defaulting to only available Python version: py3\n",
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "Defaulting to only supported image scope: cpu.\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:0.90-1-cpu-py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "training_image = retrieve(\"xgboost\", boto3.Session().region_name, \"0.90-1\")\n",
    "print(training_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a665f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "s3_train_data = \"s3://{}/{}/{}\".format(bucket, input_preprocessed_prefix_abalone, \"train/part\")\n",
    "s3_validation_data = \"s3://{}/{}/{}\".format(bucket, input_preprocessed_prefix_abalone, \"validation/part\")\n",
    "s3_output_location = \"s3://{}/{}/{}\".format(bucket, prefix, \"xgboost_model\")\n",
    "\n",
    "xgb_model = sagemaker.estimator.Estimator(\n",
    "    training_image,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    volume_size=20,\n",
    "    max_run=3600,\n",
    "    input_mode=\"File\",\n",
    "    output_path=s3_output_location,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "xgb_model.set_hyperparameters(\n",
    "    objective=\"reg:linear\",\n",
    "    eta=0.2,\n",
    "    gamma=4,\n",
    "    max_depth=5,\n",
    "    num_round=10,\n",
    "    subsample=0.7,\n",
    "    silent=0,\n",
    "    min_child_weight=6,\n",
    ")\n",
    "\n",
    "train_data = TrainingInput(\n",
    "    s3_train_data, distribution=\"FullyReplicated\", content_type=\"text/csv\", s3_data_type=\"S3Prefix\"\n",
    ")\n",
    "validation_data = TrainingInput(\n",
    "    s3_validation_data,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"text/csv\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "\n",
    "data_channels = {\"train\": train_data, \"validation\": validation_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "707532fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating training-job with name: sagemaker-xgboost-2023-01-04-11-17-42-842\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2023-01-04-11-17-42-842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-04 11:17:43 Starting - Starting the training job...\n",
      "2023-01-04 11:18:09 Starting - Preparing the instances for training...............\n",
      "2023-01-04 11:20:19 Downloading - Downloading input data...\n",
      "2023-01-04 11:21:14 Training - Training image download completed. Training in progress.\n",
      "2023-01-04 11:21:14 Uploading - Uploading generated training model.\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value reg:linear to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[11:21:09] 3362x9 matrix with 30258 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[11:21:09] 815x9 matrix with 7335 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 3362 rows\u001b[0m\n",
      "\u001b[34mINFO:root:Validation matrix has 815 rows\u001b[0m\n",
      "\u001b[34m[11:21:09] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\u001b[0m\n",
      "\u001b[34m[0]#011train-rmse:8.12284#011validation-rmse:8.01731\u001b[0m\n",
      "\u001b[34m[1]#011train-rmse:6.6449#011validation-rmse:6.56683\u001b[0m\n",
      "\u001b[34m[2]#011train-rmse:5.49491#011validation-rmse:5.44557\u001b[0m\n",
      "\u001b[34m[3]#011train-rmse:4.58894#011validation-rmse:4.56231\u001b[0m\n",
      "\u001b[34m[4]#011train-rmse:3.8966#011validation-rmse:3.89027\u001b[0m\n",
      "\u001b[34m[5]#011train-rmse:3.36039#011validation-rmse:3.35859\u001b[0m\n",
      "\u001b[34m[6]#011train-rmse:2.96475#011validation-rmse:2.97232\u001b[0m\n",
      "\u001b[34m[7]#011train-rmse:2.68478#011validation-rmse:2.70893\u001b[0m\n",
      "\u001b[34m[8]#011train-rmse:2.46846#011validation-rmse:2.51232\u001b[0m\n",
      "\u001b[34m[9]#011train-rmse:2.31318#011validation-rmse:2.37087\u001b[0m\n",
      "\n",
      "2023-01-04 11:21:31 Completed - Training job completed\n",
      "Training seconds: 72\n",
      "Billable seconds: 72\n"
     ]
    }
   ],
   "source": [
    "xgb_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228724fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
